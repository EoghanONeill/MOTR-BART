---
title: 'Implementation in R: Bayesian linear models'
author:
- Estev√£o Prado
- Hamilton Institute, Maynooth University, Ireland
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: yes
header-includes:
- \usepackage{algorithm}
- \usepackage[margin=1in]{geometry}
- \usepackage{amsmath}
- \usepackage{amsthm,amssymb}
---

<style>
body {
text-align: justify}
</style>

\section{Introduction}
Before introducing Bayesian linear regression, in this Section we present the classical linear regression model and parameter estimation via maximum likelihood in order to show the different aspects between the classical and Bayesian approaches.



\subsection{Linear regression}
Let $\{Y_{i}\}_{i=1}^{n}$ be independent and identically distributed random variables, $\{y_{i}\}_{i=1}^{n}$ their observed values and $\{x_{ij}\}_{j=1}^{d}$ the $j$-th explanatory variable for $i$. Consider $\textbf{y} = (y_{1},...,y_{n})^\top$ an $n \times 1$ column vector and $\textbf{X} = (x_{i1},...,x_{id})_{i=1}^{n}$ an $n \times d$ design matrix containing all variables that may be associated to the response variable $\textbf{y}$. The classical linear regression model may be written
\begin{align}
\textbf{y} = \textbf{X}\mathbf{\beta}  + \boldsymbol\epsilon, \mbox{ where }\boldsymbol\epsilon \sim \mbox{N}_{n}(\textbf{0},\sigma^{2}\textbf{I}_{n}), \nonumber
\end{align}


\noindent
$\boldsymbol\beta = (\beta_{0},...,\beta_{d-1})^\top$ is the $d \times 1$ vector of parameters, $\textbf{I}_{n}$ an $n \times n$ identity matrix , and $\boldsymbol\epsilon = (\epsilon_{1},...,\epsilon_{n})^\top$ the $n \times 1$ vector of errors. In addition, the likelihood function of $\textbf{y}$ given $\boldsymbol\beta$ and $\sigma^{2}$ is defined as follows
\begin{align}
\prod_{i=1}^{n} f(Y_{i} = y_{i}|\textbf{X}, \boldsymbol\beta, \sigma^{2}) =    f_{\text{y}}(\textbf{y}|\textbf{X},\boldsymbol\beta, \sigma^{2}) & = (2\pi \sigma^{2})^{-n/2} \exp \Big\{-\frac{1}{2 \sigma^{2}} (\textbf{y} - \textbf{X}\boldsymbol\beta)^\top (\textbf{y} - \textbf{X}\boldsymbol\beta)\Big\}. \nonumber
\end{align}


\noindent
In order to make inference, the $\boldsymbol\beta$ estimates are obtained by maximizing the likelihood function or its log. For mathematical convenience, once the parameter values that maximize both functions are the same, it is commonly utilized the log-likelihood, which is given by
\begin{align}
\label{Loglik_func}
\mbox{ln} f_{\text{y}}(\textbf{y}|\textbf{X}, \boldsymbol\beta, \sigma^{2}) = -\frac{n}{2} \mbox{ln} (2\pi) - \frac{n}{2} \mbox{ln} (\sigma^{2}) - \frac{1}{2 \sigma^{2}}(\textbf{y} - \textbf{X}\boldsymbol\beta)^\top (\textbf{y} - \textbf{X}\boldsymbol\beta).
\end{align}


\noindent
Considering that $\textbf{y}^\top \textbf{X}\boldsymbol\beta = \boldsymbol\beta^\top \textbf{X}^\top \textbf{y} = (\textbf{y}^\top \textbf{X}\boldsymbol\beta)^\top$ is a scalar, differentiating equation \eqref{Loglik_func} with respect to $\boldsymbol\beta$ and solving for $\boldsymbol\beta$, we have
\begin{align}
\frac{\partial \mbox{ ln} f_{\text{y}}(\textbf{y}|\textbf{X}\boldsymbol\beta, \sigma^{2})}{ \partial \boldsymbol\beta} & = \frac{1}{2 \sigma^{2}} 2\textbf{X}^\top \textbf{y} - 2 \textbf{X}^\top \textbf{X}\hat{\boldsymbol\beta} = \textbf{0} \nonumber \\
\hat{\boldsymbol\beta} & =  (\textbf{X}^\top \textbf{X})^{-1} \textbf{X}^\top \textbf{y},\nonumber
\end{align}


\noindent
the maximum likelihood (ML) estimator. Similarly, the ML estimator for $\sigma^{2}$ is given by
\begin{align}
\frac{\partial \mbox{ ln} f_{\text{y}}(\textbf{y}|\textbf{X}\boldsymbol\beta, \sigma^{2})}{ \partial \sigma^{2}} & = -\frac{n}{2 \hat{\sigma}^{2}} + \frac{(\textbf{y} - \textbf{X}\boldsymbol\beta)^\top (\textbf{y} - \textbf{X}\boldsymbol\beta)}{2 (\hat{\sigma}^{2})^{2}} = 0, \nonumber \\
& = \hat{\sigma}^{2} n + (\textbf{y} - \textbf{X}\boldsymbol\beta)^\top (\textbf{y} - \textbf{X}\boldsymbol\beta) = 0, \nonumber \\
& \hat{\sigma}^{2} = \frac{(\textbf{y} - \textbf{X}\boldsymbol\beta)^\top (\textbf{y} - \textbf{X}\boldsymbol\beta)}{n} \nonumber.
\end{align}




\noindent
Also, there are some statistical properties such as consistency that are verified for $\boldsymbol\beta$. For instance, $\hat{\boldsymbol\beta}$ is an asymptotically consistent estimator for $\boldsymbol\beta$ if $P(|\hat{\boldsymbol\beta} - \boldsymbol\beta| > a) \rightarrow 0$ as n $\rightarrow \infty$ for every $a >$ 0. For finite samples, it is consistent when $\mathbb{E} [\hat{\boldsymbol\beta}] = \boldsymbol\beta$. That is,
\begin{align}
\mathbb{E} [\hat{\boldsymbol\beta}] & = \mathbb{E} [(\textbf{X}^\top \textbf{X})^{-1} \textbf{X}^\top \textbf{y}], \nonumber\\
& = \mathbb{E} [(\textbf{X}^\top \textbf{X})^{-1} \textbf{X}^\top (\textbf{X}\boldsymbol\beta + \boldsymbol\epsilon)], \nonumber\\
& = \mathbb{E} [(\textbf{X}^\top \textbf{X})^{-1} \textbf{X}^\top \textbf{X}\boldsymbol\beta + (\textbf{X}^\top \textbf{X})^{-1} \textbf{X}^\top \boldsymbol\epsilon], \nonumber\\
& = \boldsymbol\beta, \nonumber
\end{align}


\noindent
since $(\textbf{X}^\top \textbf{X})^{-1} \textbf{X}^\top \textbf{X} = \textbf{I}$ and $\mathbb{E}[\boldsymbol\epsilon] = \textbf{0}$. Furthermore, the variance of $\hat{\boldsymbol\beta}$ is given by
\begin{align}
\mbox{Var}[\hat{\boldsymbol\beta}] & = \mbox{Var}[(\textbf{X}^\top \textbf{X})^{-1} \textbf{X}^\top \textbf{y}] \nonumber \\
& = (\textbf{X}^\top \textbf{X})^{-1} \textbf{X}^\top \mbox{ Var}[\textbf{y}] ((\textbf{X}^\top \textbf{X})^{-1} \textbf{X}^\top)^\top \nonumber \\
& = \sigma^{2}(\textbf{X}^\top \textbf{X})^{-1} \textbf{X}^\top \textbf{X} (\textbf{X}^\top \textbf{X})^{-1} \nonumber \\
& = \sigma^{2}(\textbf{X}^\top \textbf{X})^{-1}. \nonumber
\end{align}

\noindent
In Bayesian linear regression, the parameters $\boldsymbol\beta$ and $\sigma^{2}$ are estimated in a different way and usually through stochastic simulation methods. For instance, in the classical context the asymptotic distribution for $\hat{\boldsymbol\beta}$ will always be $\mbox{N}_{d}(\boldsymbol\beta,\sigma^{2}(\textbf{X}^\top \textbf{X})^{-1})$, since the Central Limit Theorem holds this result \cite[page 244]{Feller:1967}. On the other hand, in the Bayesian context the distribution of $\hat{\boldsymbol\beta}$ will not necessarily be Normal, since it depends on the choice of the prior distribution.

\subsection{Bayesian Linear regression}
In this Section we present the Bayesian linear regression models. Also, we introduce the Bayesian perspective, prior distributions and estimation methods.

\subsubsection{Basics concepts of Bayesian inference}
Under the Bayesian perspective, we aim, both through data and subjective information, to draw conclusions about a certain unknown quantity of interest by using probabilistic models. Under the classical point of view, the inference is also made utilizing probabilistic models, but only the information from the data is considered and any other extra information is not incorporated into the decision process.


The inclusion of the subjective information in the inference process is the main point in Bayesian analysis. That is made by inserting a prior distribution that describes all the available knowledge that one can have about the quantity of interest, and its choice may influence the final results depending on how much data is available. That is, the more data, the less the impact of the prior information on the final conclusions \cite{Gelmanetal:2014}.

There are many types of prior distributions, such as non-informative, conjugate, improper etc. The non-informative ones are based on the sampling distribution and their idea is to have a default prior distribution when there is no information about the problem at hand. For instance, the Jeffreys prior is non-informative and it is proportional to the Fisher Information, which is the expected value of the second derivative of the log-likelihood function with respect to the parameter of interest \cite{CRobert:2014}. Although the Jeffreys prior is called non-informative, the Fisher Information quantify the variability of the parameter based on the available data. That is, the higher the value of the Fisher Information, the more concave is the log-likelihood, thus evidencing that the data helps to estimate the quantity of interest.

The conjugate priors are a class of distributions that present the same parametric form of the likelihood function and their choice is frequently related to mathematical and computational convenience \cite{CRobert:2014}. As a consequence of conjugacy, the posterior distribution may be obtained analytically and posterior samples are generated straightforwardly. On the other hand, improper priors are distributions that, in their parametric space, do not integrate to 1. For instance, in some cases Jeffreys priors are improper, but the posterior distribution is proper; see Section 3.2 of \cite{Gelmanetal:2014}.

Consider $\boldsymbol\theta = (\theta_{0},...,\theta_{d-1})^\top$ an unknown vector of parameters that we are interested in estimating and $\textbf{y} = (y_{1},...,y_{n})^\top$ an $n \times 1$ column vector assumed to be a realization of the random variable whose distribution is $p(y_{i}|\boldsymbol\theta)$. The likelihood function of the $y_{i}$ is given by

\begin{align}
\label{Lik_func}
    \mathcal{L}(\boldsymbol\theta| \textbf{y}) = \prod_{i=1}^{n} p(y_{i}|\boldsymbol\theta).
\end{align}

\noindent
All the information from the observations $y_{i}$ about $\boldsymbol\theta$ is included in \eqref{Lik_func}. The difficulty in estimating $\boldsymbol\theta$ becomes an optimization problem of maximizing the likelihood function (or its logarithm). In constrast, under the Bayesian methodology the estimate of $\boldsymbol\theta$ is given by the joint posterior distribution, which is defined by the Bayes' theorem,
\begin{align}
\label{Post_func}
    p(\boldsymbol\theta|\textbf{y}) = \frac{\mathcal{L}(\theta| \textbf{y}) p(\boldsymbol\theta)}{\int_{\Theta} \mathcal{L}(\theta| \textbf{y}) p(\boldsymbol\theta) d \boldsymbol\theta},
\end{align}

\noindent
where $\Theta$ represents the parametric space of $\boldsymbol\theta$ and $p(\boldsymbol\theta)$ the prior distribution. Equation \eqref{Post_func} can also be written as
\begin{align}
\label{Post_prop}
    p(\boldsymbol\theta|\textbf{y}) \varpropto \mathcal{L}(\theta| \textbf{y}) p(\boldsymbol\theta),
\end{align}

\noindent
since $\int_{\Theta} \mathcal{L}(\theta| \textbf{y}) p(\boldsymbol\theta) d \boldsymbol\theta$ is the marginal distribution of $\textbf{y}$ and does not depend on $\boldsymbol\theta$. The posterior distribution $p(\boldsymbol\theta|\textbf{y})$ provides all the information that one can have about $\boldsymbol\theta$. For instance, it is possible to evaluate $p(\boldsymbol\theta|\textbf{y})$ and its mean, median, variance and some other quantities such as quantiles in order to have point and interval estimates. Besides, the posterior distribution frequently has no closed form, thus depending on computational methods to be obtained.

When the posterior distribution is available, one can be interested about the predictive posterior distribution, which is utilized to predict unobserved values of the response outcome, $\tilde{\textbf{y}}$, and the marginal distribution of $\textbf{y}$. To obtain these two distributions, the constant that was not considered in \eqref{Post_prop} is necessary.
\begin{align}
p(\tilde{\textbf{y}}|\textbf{y}) & = {\int_{\Theta} p(\tilde{\textbf{y}}|\boldsymbol\theta,\textbf{y}) p(\boldsymbol\theta|\textbf{y}) d\boldsymbol\theta}, \mbox{          } \mbox{          } \mbox{          } \mbox{          }\mbox{          }\tilde{\textbf{y}} \sim  p(\tilde{\textbf{y}}|\boldsymbol\theta,\textbf{y}), \nonumber \\
p(\textbf{y}) & = {\int_{\Theta} \mathcal{L}(\boldsymbol\theta|\textbf{y}) p(\boldsymbol\theta) d\boldsymbol\theta}. \nonumber
\end{align}

\noindent
The advantages of the Bayesian inference when compared to the classical one are that all the information available is considered and its probabilistic interpretation about the estimates is straightforward \cite{AOHagan:1994}. The prior knowledge is inserted through the prior distribution and combined with the data, represented by the likelihood function, all inference is carried out based on the posterior distribution. In parallel, the interpretation of the interval estimates does not involve assumptions about replications of the experiment. That is, in the Bayesian context given the interval estimates, $\boldsymbol\theta$ belongs to it with $(1-\alpha)$\% probability.

\subsubsection{Bayesian Linear model: conjugate priors}

Here we consider a Bayesian linear model in the form
 \begin{align}
     \textbf{y} = \textbf{X}\beta  + \boldsymbol\epsilon, \mbox{ where }\boldsymbol\epsilon \sim \mbox{N}_{n}(\textbf{0},\sigma^{2}\textbf{I}_{d}), \nonumber
\end{align}


where $\sigma^{2} > 0$, $\textbf{I}_{d}$ an identity matrix, $\boldsymbol\beta = (\beta_{0},...,\beta_{d-1})^\top$ a $d \times 1$ vector, $\textbf{X}$ an $n \times d$ design matrix and we assume that $\epsilon_{i}$'s are independent. The likelihood function is also



\begin{align}
f_{\text{y}}(\textbf{y}|\textbf{X}, \boldsymbol\beta, \sigma^{2}) & = (2\pi \sigma^{2})^{-n/2} \exp \Big\{-\frac{1}{2 \sigma^{2}} (\textbf{y} - \textbf{X}\boldsymbol\beta)^\top (\textbf{y} - \textbf{X}\boldsymbol\beta)\Big\} \nonumber
\end{align}


Under the Bayesian point of view, the inference process involves data and prior information. Thus, we assume a $\textbf{N}_{d}(\textbf{m}, \sigma^{2} \textbf{V})$, which is a conjugate prior distribution for $\boldsymbol\beta | \sigma^{2}$ as follows



\begin{align}
f(\boldsymbol\beta|\sigma^{2}, \textbf{m}, \textbf{V}) & = (2\pi \sigma^{2})^{-d/2} |\textbf{V}|^{-1/2} \exp \Big\{-\frac{1}{2 \sigma^{2}} (\boldsymbol\beta - \textbf{m})^\top \textbf{V}^{-1} (\boldsymbol\beta - \textbf{m})\Big\}. \nonumber
\end{align}


For $\sigma^{2}$, we also set a conjugate prior distribution given by an Inverse Gamma denoted by $\mbox{IG}(a,b)$ in the form of



\begin{align}
f(\sigma^{2}| a, b) & = \frac{b^{a}}{\Gamma(a)} (\sigma^{2})^{-(a+1)} \exp \Big\{-\frac{b}{\sigma^{2}}\Big\}, \nonumber
\end{align}
where $a > 0$ and $b >0$. Since we have the likelihood function and the proper priors, we can then find the posterior distribution to make inference on the parameters $\boldsymbol\beta$ and $\sigma^{2}$. Using the Bayes' theorem, we have



\begin{align}
\label{P1_post_dist}
    f(\boldsymbol\beta, \sigma^{2}| \textbf{y}, \textbf{X}) & = \frac{f_{\text{y}}(\textbf{y}|\textbf{X}, \boldsymbol\beta, \sigma^{2}) f(\boldsymbol\beta|\sigma^{2},\textbf{m}, \textbf{V})  f(\sigma^{2}| a, b)}{  f_{\text{y}}(\textbf{y})}, \\
    & \varpropto f_{\text{y}}(\textbf{y}|\textbf{X}, \boldsymbol\beta, \sigma^{2}) f(\boldsymbol\beta|\sigma^{2},\textbf{m}, \textbf{V})  f(\sigma^{2}| a, b), \nonumber \\ 
    & \varpropto (\sigma^{2})^{-\frac{n}{2} - \frac{d}{2} + a - 1} \exp \Big\{-\frac{A}{2 \sigma^{2}} \Big\}, \nonumber
\end{align}

where
\begin{align}
    A & =  (\textbf{y} - \textbf{X}\boldsymbol\beta)^\top (\textbf{y} - \textbf{X}\boldsymbol\beta) + (\boldsymbol\beta - \textbf{m})^\top \textbf{V}^{-1} (\boldsymbol\beta - \textbf{m}) + 2b, \nonumber \\
      & =  \textbf{y}^\top \textbf{y} - \textbf{y}\textbf{X} \boldsymbol\beta - \boldsymbol\beta^\top \textbf{X}^\top \textbf{y} + \boldsymbol\beta^\top \textbf{X}^\top \textbf{X} \boldsymbol\beta + \boldsymbol\beta^\top \textbf{V}^{-1} \boldsymbol\beta - \boldsymbol\beta^\top \textbf{V}^{-1} \textbf{m} - \textbf{m}^\top \textbf{V}^{-1} \boldsymbol\beta + \textbf{m}^\top \textbf{V}^{-1}\textbf{m} + 2b, \nonumber \\
     & =  \boldsymbol\beta^\top (\textbf{X}^\top \textbf{X} + \textbf{V}^{-1}) \boldsymbol\beta - \boldsymbol\beta^\top(\textbf{X}^\top \textbf{y} + \textbf{V}^{-1} \textbf{m}) + (\textbf{m}^\top \textbf{V}^{-1} \textbf{m} + 2b + \textbf{y}^\top \textbf{y}) -( \textbf{y}^\top \textbf{X} + \textbf{m}^\top \textbf{V}^{-1})\boldsymbol\beta \nonumber.
\end{align}


For convenience, let $\boldsymbol\Lambda = (\textbf{X}^\top \textbf{X} + \textbf{V}^{-1})^{-1}$ a $d \times d$ matrix and $\boldsymbol\mu = (\textbf{X}^\top \textbf{X} + \textbf{V}^{-1})^{-1} (\textbf{X}^\top \textbf{y} + \textbf{V}^{-1}\textbf{m})$ a $d \times 1$ vector. Hence,


\begin{align}
    A & = \boldsymbol\beta^\top \boldsymbol\Lambda^{-1} \boldsymbol\beta - \boldsymbol\beta^\top \boldsymbol\Lambda^{-1} \boldsymbol\mu - \boldsymbol\mu^\top \boldsymbol\Lambda^{-1} \boldsymbol\beta + \textbf{m}^\top \textbf{V}^{-1} \textbf{m} + 2b + \textbf{y}^\top \textbf{y}, \nonumber \\
    & = (\boldsymbol\beta - \boldsymbol\mu)^\top \boldsymbol\Lambda^{-1} (\boldsymbol\beta - \boldsymbol\mu) - \boldsymbol\mu^\top \boldsymbol\Lambda^{-1} \boldsymbol\mu + \textbf{m}^\top \textbf{V}^{-1} \textbf{m} + 2b + \textbf{y}^\top \textbf{y}. \nonumber
\end{align}


Finally, the joint posterior distribution for $\boldsymbol\beta$ and $\sigma^{2}$ is given by



\begin{align}
\label{Post_dist}
f(\boldsymbol\beta, \sigma^{2}| \textbf{y}, \textbf{X})   \varpropto & f_{\text{y}}(\textbf{y}|\textbf{X}, \boldsymbol\beta, \sigma^{2}) f(\boldsymbol\beta|\sigma^{2},\textbf{m}, \textbf{V})  f(\sigma^{2}| a, b), \nonumber \\ 
\varpropto & (\sigma^{2})^{- \frac{d}{2}} \exp \Big\{-\frac{(\boldsymbol\beta - \boldsymbol\mu)^\top \boldsymbol\Lambda^{-1} (\boldsymbol\beta - \boldsymbol\mu)}{2 \sigma^{2}} \Big\} \nonumber \\
& \times (\sigma^{2})^{-\frac{n}{2} + a - 1} \exp \Big\{-\frac{\textbf{m}^\top \textbf{V}^{-1} \textbf{m} -\boldsymbol\mu^\top \boldsymbol\Lambda^{-1} \boldsymbol\mu + 2b + \textbf{y}^\top \textbf{y}}{2 \sigma^{2}} \Big\}.
\end{align}



Therefore, the equation \eqref{Post_dist} shows that the posterior distribution $f(\boldsymbol\beta, \sigma^{2}| \textbf{y}, \textbf{X})$ is proportional to the multiplication of kernels of the $\textbf{N}_{n}(\boldsymbol\mu, \sigma^{2}\boldsymbol\Lambda)$ and IG ($a^{*} = \frac{n}{2} + a$, $b^{*} =  b + \frac{\textbf{m}^\top \textbf{V}^{-1} \textbf{m} -\boldsymbol\mu^\top \boldsymbol\Lambda^{-1} \boldsymbol\mu + \textbf{y}^\top \textbf{y}}{2}$). The manipulations presented above are partially available in \cite{Gelmanetal:2014}, \cite{AOHagan:1994} and \cite{Gamerman:Lopes:2006}. Additionally, \cite{AOHagan:1994} shows the normalizing constant and the marginal distribution of $\textbf{y}$, which are necessary for equation \eqref{P1_post_dist}. Below, we implement the Gibbs Sampling for this example. 

```{r, echo=TRUE, warning=FALSE, results='hide', fig.align='center', out.width='80%', out.height='80%'}

# ----------------------------------------------------------- #
# Description: Gibbs Sampling for Bayesian linear regression  #
#              with conjugate priors both for betas and sigma #
# Author: Estev√£o Prado                                       #
# Last modification: 22/10/2018                               #
# ----------------------------------------------------------- #

require(MASS)
require(ggplot2)
require(gridExtra)
set.seed(12345)

## MCMC simulation: example 1 (Gibbs sampling)
## -------------------------------------------

## Defining simulation size, burn-in, etc
## --------------------------------------
Niter <- 500
BurnIn <- 200
TotIter <- Niter+BurnIn
N <- 1000
b0 <- 1.4
b1 <- -0.8
b2 <- 1.5
b3 <- 0.5
betas <- c(b0,b1,b2,b3)
sigma <- 4
AuxBurnIn <- 1

## Simulation scheme 
## -----------------
x1 <- rbinom(n = N, prob = 0.5, size = 1)
x2 <- rpois(n = N, lambda = 5)
x3 <- round(rnorm(n = N, mean = 0, sd = 3),2)
e <- rnorm(n = N, mean = 0, sd = sqrt(sigma))
X <- cbind(rep(1,N),x1, x2, x3)
y <- rnorm(n = N, mean = betas[1] + betas[2]*x1 + betas[3]*x2 + betas[4]*x3 + e)

## Defining prior distributions
## ----------------------------
## Betas ~ Normal(m, V)
## --------------------
m = rbind(0,0,0,0)
V = diag(4)

## Sigma2 ~ Inverse Gamma(a, b)
## ---------------------------
a = 2
b = 1

## Posterior parameters
## --------------------
mu <- solve(t(X)%*%X + solve(V)) %*% (t(X)%*%y + solve(V)%*%m); mu
Lambda <- solve(t(X)%*%X + solve(V)); Lambda
a_ast <- (N/2) + a; a_ast
b_ast <- b + 0.5*(-t(mu)%*%solve(Lambda)%*%mu + t(m)%*%solve(V)%*%m + t(y)%*%y); b_ast

## Data frame that will store MCMC values for betas and sigma2
## -----------------------------------------------------------
SaveResults <- as.data.frame(matrix(data = NA, nrow = Niter, ncol = length(betas)+2))
colnames(SaveResults) <- c('Iter', 'Beta0', 'Beta1', 'Beta2', 'Beta3', 'sigma2')

## Getting started Gibbs Sampling
## ------------------------------
for(i in 1:TotIter){

  MCMCSigma <- 1/rgamma(1, a_ast, b_ast)
  MCMCBetas <- mvrnorm(1, mu, MCMCSigma*Lambda)

  if (i > BurnIn){
    SaveResults[AuxBurnIn,] <- c(AuxBurnIn,MCMCBetas, MCMCSigma)
    AuxBurnIn <- AuxBurnIn + 1
  }
}

## True values
## -----------
betas
sigma

## Plots
## -----
ChainB0 <- ggplot(SaveResults, aes(x=Iter)) +
  geom_line(aes(y=Beta0)) +
  labs(title=expression(paste('Chain of ', beta[0])),
       subtitle='Conjugate priors: Gibbs Sampling',
       x='Iterations',
       y='Values') +
  geom_hline(yintercept=betas[1], linetype = 'dotted', color='coral') +
  theme_bw()

ChainB1 <- ggplot(SaveResults, aes(x=Iter)) +
  geom_line(aes(y=Beta1)) +
  labs(title=expression(paste('Chain of ', beta[1])),
       subtitle='Conjugate priors: Gibbs Sampling',
       x='Iterations',
       y='Values') +
  geom_hline(yintercept=betas[2], linetype = 'dotted', color='coral') +
  theme_bw()

ChainB2 <- ggplot(SaveResults, aes(x=Iter)) +
  geom_line(aes(y=Beta2)) +
  labs(title=expression(paste('Chain of ', beta[2])),
       subtitle='Conjugate priors: Gibbs Sampling',
       x='Iterations',
       y='Values') +
  geom_hline(yintercept=betas[3], linetype = 'dotted', color='coral') +
  theme_bw()

ChainB3 <- ggplot(SaveResults, aes(x=Iter)) +
  geom_line(aes(y=Beta3)) +
  labs(title=expression(paste('Chain of ', beta[3])),
       subtitle='Conjugate priors: Gibbs Sampling',
       x='Iterations',
       y='Values') +
  geom_hline(yintercept=betas[4], linetype = 'dotted', color='coral') +
  theme_bw()

ChainS2 <- ggplot(SaveResults, aes(x=Iter)) +
  geom_line(aes(y=sigma2)) +
  labs(title=expression(paste('Chain of ', sigma^{2})),
       subtitle='Conjugate priors: Gibbs Sampling',
       x='Iterations',
       y='Values') +
  geom_hline(yintercept=sigma, linetype = 'dotted', color='coral') +
  theme_bw()

print(ChainB0)
print(ChainB1)
print(ChainB2)
print(ChainB3)
print(ChainS2)

#grid.arrange(ChainB0, ChainB1, ChainB2, ChainB3, ChainS2, ncol=1)
```


\subsubsection{Bayesian Linear model: non-conjugate prior}

In order to illustrate a Bayesian linear model with non-conjugate prior where Metropolis-Hastings may be helpful, consider $\textbf{y} \sim \textbf{N}_{n}(\textbf{X}\boldsymbol\beta, \sigma^{2} \textbf{I})$ and assume that $\sigma^{2}$ is known. Also, consider a Laplace distribution as prior for each $\beta_{i}$ such as



\begin{align}
     f(\beta_{i} | m_{i}, v_{i}) = (2v_{i})^{-d/2} \exp \Big\{- \sum_{i=1}^{d} \frac{|\beta_{i} - m_{i}|}{v_{i}}\Big\}, \nonumber
\end{align}


where $\beta_{i} \in \mathbb{R}$, $m_{i} \in \mathbb{R}$ and $v_{i} > 0$. In this case, the posterior distribution is given by



\begin{align}
    f(\boldsymbol\beta | \textbf{y}, \textbf{X}, \sigma^{2}) & \varpropto f_{\text{y}}(\textbf{y}|\textbf{X}, \boldsymbol\beta) \prod_{i=1}^{d} f(\beta_{i} | m_{i}, v_{i}), \nonumber \\
    & \varpropto \exp \Big\{-\frac{1}{2 \sigma^{2}} (\textbf{y} - \textbf{X}\boldsymbol\beta)^\top (\textbf{y} - \textbf{X}\boldsymbol\beta) - \sum_{i=1}^{d} \frac{|\beta_{i} - m_{i}|}{v_{i}} \Big\}. \nonumber
\end{align}


The equation above has no closed form of a known p.d.f or p.m.f and MCMC methods can be utilized in order to obtain samples from the posterior distributions of $\beta_{i}$. Although the Gibbs Sampling is an MCMC method, in this case it cannot be used since it is not possible to sample directly from $f(\boldsymbol\beta | \textbf{y}, \textbf{X}, \sigma^{2})$. In contrast, Metropolis-Hasting can be applied and its implementation is given below.


```{r, echo=TRUE, warning=FALSE, results='hide', fig.align='center', out.width='80%', out.height='80%'}

# ---------------------------------------------------------------- #
# Description: Metropolis-Hastings for Bayesian linear regression  #
#              with a Laplace distribution as prior for betas.     #
#              In this example, sigma is assumed to be known.      #
# Author: Estev√£o Prado                                            #
# Last modification: 22/10/2018                                    #
# ---------------------------------------------------------------- #

require(MASS)
require(ggplot2)
require(gridExtra)
set.seed(12345)

## MCMC simulation: example 2 (Metropolis-Hastings)
## ------------------------------------------------

## Defining simulation size, burn-in, etc
## --------------------------------------

Niter <- 10000
BurnIn <- 100
TotIter <- Niter+BurnIn
N <- 1000
b0 <- 1.4
b1 <- -0.8
b2 <- 1.5
b3 <- 0.5
betas <- c(b0,b1,b2,b3)
sigma <- 4
AuxBurnIn <- 1

## Simulation scheme 
## -----------------
x1 <- rbinom(n = N, prob = 0.5, size = 1)
x2 <- rpois(n = N, lambda = 5)
x3 <- round(rnorm(n = N, mean = 0, sd = 4),2)
e <- rnorm(n = N, mean = 0, sd = sqrt(sigma))
X <- cbind(rep(1,N),x1, x2, x3)
y <- rnorm(n = N, mean = betas[1] + betas[2]*x1 + betas[3]*x2 + betas[4]*x3 + e)

## Defining prior distributions
## ----------------------------
## Betas ~ Laplace(m_{i}, v_{i})
## ----------------------------
m = rbind(0,0,0,0)
v = rbind(1,1,1,1)

## Data frame that will store MCMC values for betas
## ------------------------------------------------
SaveResults <- as.data.frame(matrix(data = NA, nrow = Niter, ncol = length(betas)+1))
colnames(SaveResults) <- c('Iter', 'Beta0', 'Beta1', 'Beta2', 'Beta3')

## Initial values for Betas and covariance matrix of the proposal distribution
## ---------------------------------------------------------------------------
MCMCBetasI <- c(10,10,10,10)
V = diag(4)*0.0005

## Getting started Metropolis-Hastings
## -----------------------------------
for(i in 1:TotIter){
  
  MCMCBetasC <- mvrnorm(1, MCMCBetasI, V)

  razao <- (-0.5*sigma*(t(y-X%*%MCMCBetasC)%*%(y-X%*%MCMCBetasC) - sum(abs(MCMCBetasC - m)/v))) - 
           (-0.5*sigma*(t(y-X%*%MCMCBetasI)%*%(y-X%*%MCMCBetasI) - sum(abs(MCMCBetasI - m)/v)))
  
  if(runif(1) < min(1, exp(razao)))
    {MCMCBetasI <- MCMCBetasC}
  
  if (i > BurnIn){
    SaveResults[AuxBurnIn,] <- c(AuxBurnIn,MCMCBetasI)
    AuxBurnIn <- AuxBurnIn + 1
  }
}

head(SaveResults)

## True values
## -----------
betas

## Plots
## -----
ChainB0 <- ggplot(SaveResults, aes(x=Iter)) +
  geom_line(aes(y=Beta0)) +
  labs(title=expression(paste('Chain of ', beta[0])),
       subtitle='Non-conjugate priors: Metropolis-Hastings',
       x='Iterations',
       y='Values') +
  geom_hline(yintercept=betas[1], linetype = 'dotted', color='coral') +
  theme_bw()

ChainB1 <- ggplot(SaveResults, aes(x=Iter)) +
  geom_line(aes(y=Beta1)) +
  labs(title=expression(paste('Chain of ', beta[1])),
       subtitle='Non-conjugate priors: Metropolis-Hastings',
       x='Iterations',
       y='Values') +
  geom_hline(yintercept=betas[2], linetype = 'dotted', color='coral') +
  theme_bw()

ChainB2 <- ggplot(SaveResults, aes(x=Iter)) +
  geom_line(aes(y=Beta2)) +
  labs(title=expression(paste('Chain of ', beta[2])),
       subtitle='Non-conjugate priors: Metropolis-Hastings',
       x='Iterations',
       y='Values') +
  geom_hline(yintercept=betas[3], linetype = 'dotted', color='coral') +
  theme_bw()

ChainB3 <- ggplot(SaveResults, aes(x=Iter)) +
  geom_line(aes(y=Beta3)) +
  labs(title=expression(paste('Chain of ', beta[3])),
       subtitle='Non-conjugate priors: Metropolis-Hastings',
       x='Iterations',
       y='Values') +
  geom_hline(yintercept=betas[4], linetype = 'dotted', color='coral') +
  theme_bw()

print(ChainB0)
print(ChainB1)
print(ChainB2)
print(ChainB3)

#grid.arrange(ChainB0, ChainB1, ChainB2, ChainB3, ncol=2)
```

\subsubsection{MCMC methods}

In order to estimate the unknown quantities in the Bayesian models, Markov Chain Monte Carlo (MCMC) methods are useful tools to sample from those posterior distributions that have no closed form. In this section we briefly introduce two MCMC algorithms commonly used: Gibbs Sampling \cite{Geman:Geman:1984, Gelfand:Smith:1990} and Metropolis-Hastings \cite{Metropolisetal:1953, Hastings:1970}.


In the statistical context, Monte Carlo integration is convenient when it is not possible to analytically compute a finite integral such as


\begin{equation}\label{Eq1:MonteCarlo}
\int g(\theta) p(\theta) d\theta,
\end{equation}



\noindent
where $p(\cdot)$ is a p.d.f or p.m.f and $g(\cdot)$ a integrable function. In short, i.i.d samples are generated from $p(\cdot)$ and evaluated at $g(\cdot)$ and then averaged. For a sufficiently large number of samples, the Strong Law of Large Numbers holds this average converges almost surely to \eqref{Eq1:MonteCarlo} \cite{Robert:Casella:2004}. When it is not possible to directly sample from $p(\cdot)$, Gibbs Sampling and Metropolis-Hastings are alternatives to generate samples $\theta^{(1)}, \theta^{(2)}, ..., \theta^{(K)}$ from $p(\cdot)$ no longer independent (now with a Markovian dependence structure) that evaluated at $t(\cdot)$ and averaged over the samples, also converge almost surely to \eqref{Eq1:MonteCarlo}.



The Gibbs Sampling is a stochastic simulation algorithm via Markov Chain utilized when the joint posterior distribution has no closed form but all its conditional distributions do. For instance, consider that $p(\boldsymbol\theta|\textbf{y})$ is the joint posterior distribution and suppose that its conditional distributions may be written as
\begin{equation}
p(\theta_{k}|\theta_{0}, ..., \theta_{k-1}, \theta_{k+1}, ..., \theta_{d-1},\textbf{y}), \mbox{ where } k=0,...,d-1 \nonumber.
\end{equation}


\noindent
The idea is to successively sample from each conditional distribution of $\theta_{k}$ in order to obtain samples from the joint posterior distribution. The Gibbs Sampling is described as following
\begin{algorithm}[H]{\textbf{Algorithm} {Gibbs Sampling}}



\begin{enumerate}
\item Initialize $t=1$ and define initial values
$\theta_{0}^{(0)},\theta_{1}^{(0)},..., \theta_{d-1}^{(0)}$ for the vector
$\boldsymbol\theta = (\theta_{0},\theta_{2},..., \theta_{d-1})^\top$.
\item Sample
\begin{align}
&\theta_{0}^{(t)} \sim p(\theta_{0}|\theta_{1}^{(t-1)}, \theta_{2}^{(t-1)}, 
\theta_{3}^{(t-1)},..., \theta_{d-1}^{(t-1)}, \textbf{y}); \nonumber \\
&\theta_{1}^{(t)} \sim p(\theta_{1}|\theta_{2}^{(t)}, \theta_{3}^{(t-1)}, 
\theta_{4}^{(t-1)}, ..., \theta_{d-1}^{(t-1)}, \textbf{y}); \nonumber \\
& \hspace{3.4cm}  \vdots \nonumber\\
&\theta_{d-1}^{(t)} \sim p(\theta_{d-1}|\theta_{1}^{(t)}, \theta_{2}^{(t)}, 
\theta_{3}^{(t)}, ..., \theta_{d-2}^{(t)}, \textbf{y}); \nonumber
\end{align}
\item Take $t=t+1$ and return to the step 2 until the desired sample has been obtained for each $\theta_{k}$.
\end{enumerate}
\end{algorithm}

Similarly to the Gibbs Sampling, Metropolis-Hastings is also a stochastic algorithm that generates samples with Markovian dependence structure from a certain distribution (or kernel). Further, Metropolis-Hastings is more flexible than Gibbs Sampling, since it can be utilized to generate samples from distributions that have or not closed form. When it is possible to generate directly from the conditional or joint distribution, it is appropriate to use Gibbs Sampling, once Metropolis-Hastings has an acceptance and rejection step. But when at least one of the conditional distributions have no closed form, Metropolis might be an alternative. In some cases Metropolis and Gibbs Sampling are combined because some conditionals have closed form and others do not. This hybrid algorithm is called Metropolis-within-Gibbs and was proposed by \cite{Muller:1991} and \cite{Muller:1993}.


Again, suppose we desire to obtain a sample from the joint posterior distribution $p(\boldsymbol\theta| \textbf{y})$. The Metropolis-Hastings is based on a proposal distribution $q(\boldsymbol\theta^{(t-1)})$, which generates candidate values $\boldsymbol\theta^{*}$ that are accepted as values from $p(\boldsymbol\theta| \textbf{y})$ with certain probability. In its first version \cite{Metropolisetal:1953}, the $q(\boldsymbol\theta^{(t-1)})$ is only Normal, where the mean has a Markovian structure and the variance is constant. A more general framework was proposed by \cite{Hastings:1970} in which the proposal distribution can be other than Normal, and since then many adaptive Metropolis algorithms have been introduced, which basically differ by the specification of the covariance matrix of the proposal distribution \cite{Gamerman:1997, Haario:2001, Vihola:2012, Simo:2015}. Below, the Metropolis algorithm proposed by \cite{Hastings:1970} is described. 



\begin{algorithm}[H]{\textbf{Algorithm} Metropolis-Hastings}
\begin{enumerate}
\item Initialize $t=1$ and define the initial values
$\theta_{1}^{(0)},\theta_{2}^{(0)},..., \theta_{d}^{(0)}$ for the vector 
$\boldsymbol\theta = (\theta_{0},\theta_{1},..., \theta_{d-1})$;

\item Sample $\boldsymbol\theta^{*}$ from the proposal distribution $q( 
\boldsymbol\theta^{(t-1)})$;

\begin{enumerate}

\item Compute
\begin{equation}
\alpha(\boldsymbol\theta^{(t-1)}, \boldsymbol\theta^{*}) = \mbox{min} \left \lbrace 1, 
\frac{p(\boldsymbol\theta^{*}| \textbf{y}) 
q( \boldsymbol\theta^{*})}{p(\boldsymbol\theta^{(t-1)}| 
\textbf{y}) q( \boldsymbol\theta^{(t-1)})} \right \rbrace,
\label{razaoCompleta}
\end{equation}


\item Compute $u \sim U[0,1]$. If $u < \alpha(\boldsymbol\theta^{(t-1)}, \boldsymbol\theta^{*})$, then $\boldsymbol\theta^{(t)} = \boldsymbol\theta^{*}$, otherwise, $\boldsymbol\theta^{(t)} = \boldsymbol\theta^{(t-1)}$;

\end{enumerate}

\item Take $t=t+1$ and return to the step 2 until the desired posterior sample has been obtained.

\end{enumerate}
\end{algorithm}


\noindent
The choice of the proposal distribution must be based on the support of $\boldsymbol\theta$. For example, if $\boldsymbol\theta \in \mathbb{R}^{d}$ it is appropriate to choose a proposal that is also supported on $\mathbb{R}^{d}$, otherwise the results generated by Metropolis might be invalid.


One important characteristic of the MCMC algorithms is that they generate chains that need to converge. That is, for each component of $\boldsymbol\theta$ or $\boldsymbol\beta$, a chain of values with Markovian dependence structure is generated and one must verify its convergence to the joint posterior distribution. Due to it, we usually set a warm-up period, which is called burn-in, from which the samples start being considered and all inference is made utilizing only these samples; see \cite{Robert:Casella:2004} for theoretical convergence results of the MCMC methods.



\newpage
\begin{thebibliography}{999}

\bibitem{Feller:1967}%8
W. Feller. An Introduction to Probability Theory and Its applications. 1967; John Wiley, Ed.3, vol. 1.

\bibitem{Gelmanetal:2014}%1
A. Gelman, J. Carlin, H. Stern, D. Dunson, A. Vehtari, D. Rubin. Bayesian Data Analysis. 2014; CRC, Ed.3, Boca Raton.

\bibitem{CRobert:2014}%2
C. Robert. The Bayesian Choice: from Decision-Theoretic Foundations to Computational Implementation. 2007; Springer, Ed. 2, New York.

\bibitem{AOHagan:1994}%3
A. O'Hagan. Kendall's Advanced Theory of Statistics: Bayesian Inference. 1994; Arnold, Ed. 2B, London.

\bibitem{Geman:Geman:1984}%12
S. Geman, D. Geman. Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images. IEEE Transactions Pattern Analysis and Machine Intelligence. 1984; 6:721--741.

\bibitem{Gelfand:Smith:1990}%12
A. Gelfand, A. Smith. Sampling based approaches to calculating marginal densities. Journal of American Statistical Association. 1990; 85:398--409 Springer, New York.

\bibitem{Metropolisetal:1953}%6
N. Metropolis, A. Rosenbluth, M. Teller, E. Teller. Equations of state calculations by fast computing machines. Journal of Chemistry and Physics. 1953;1087:1091--21.

\bibitem{Hastings:1970}%10
W. Hastings. Monte Carlo sampling using Markov chains and their applications. Biometrika. 1970; 57: 97-109.

\bibitem{Robert:Casella:2004}%5
C. Robert, G. Casella. Monte Carlo Statistical Methods. 2004; Springer, New York.

\bibitem{Muller:1991}%18
P. Muller. A generic approach to posterior integration and Gibbs sampling. Technical report, Purdue University, West Lafayette, Indiana.

\bibitem{Muller:1993}%19
P. Muller. Alternatives to the Gibbs Sampling scheme. Technical report, Institute of Statistics and Decision Science, Duke University, Durham, North Carolina.

\bibitem{Gamerman:1997}%4
D. Gamerman. Sampling from the posterior distribution in generalized linear mixed models. Statistics and Computing. 1997;57:68--7.

\bibitem{Haario:2001}%6
H. Haario, E. Saksman, J. Tamminen. An adaptive Metropolis algorithm. Bernoulli. 2001;223:243--7.

\bibitem{Vihola:2012}%8
M. Vihola. Robust adaptive Metropolis algorithm with coerced acceptance rate. Statistics and Computing. 2012;997:1008--843.

\bibitem{Simo:2015}%11
I. S. Mbalawata, S. Sarkka, M. Vihola, H. Haario. Adaptive Metropolis algorithm using variational Bayesian adaptive Kalman Filter. Computational Statistics and Data Analysis. 2015;101:115--83.

\bibitem{Gamerman:Lopes:2006}%9
D. Gamerman, H. F. Lopes. Markov Chain Monte Carlo: Stochastic Simulation for Bayesian Inference. 2006; Chapman and Hall/CRC, Ed. 2, London.

\end{thebibliography}
